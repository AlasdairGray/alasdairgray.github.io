@article{Akinyemi:offshore-data-integration:2020,
  abstract = {Offshore decommissioning represents significant business opportunities for oil and gas service companies. However, for owners of offshore assets and regulators, it is a liability because of the associated costs. One way of mitigating decommissioning costs is through the sales and reuse of decommissioned items. To achieve this effectively, reliability assessment of decommissioned items is required. Such an assessment relies on data collected on the various items over the lifecycle of an engineering asset. Considering that offshore platforms have a design life of about 25 years and data management techniques and tools are constantly evolving, data captured about items to be decommissioned will be in varying forms. In addition, considering the many stakeholders involved with a facility over its lifecycle, information representation of the items will have variations. These challenges make data integration difficult. As a result, this research developed a data integration framework that makes use of Semantic Web technologies and ISO 15926 - a standard for process plant data integration - for rapid assessment of decommissioned items. The proposed solution helps in determining the reuse potential of decommissioned items, which can save on cost and benefit the environment.},
	doi = {10.1016/j.autcon.2019.103010},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580518304059},
	year = 2020,
	month = nov,
	publisher = {Elsevier},
	volume = {109},
	pages = {103010},
	author = {Abiodun G. Akinyemi and Ming Sun and Alasdair J. G. Gray},
	title = {Data integration for offshore decommissioning waste management},
	journal = {Automation in Construction}
}

@article{Akinyemi_2018,
  abstract = {Information management during the construction phase of a built asset involves multiple stakeholders using multiple software applications to generate and store data. This is problematic as data come in different forms and are labour intensive to piece together. Existing solutions to this problem are predominantly in proprietary applications, which are sometimes cost prohibitive for small engineering firms, or conceptual studies with use cases that cannot be easily adapted. In view of these limitations, this research presents an ontology-based data integration framework that makes use of open-source tools that support Semantic Web technologies. The proposed framework enables rapid answering of queries over construction data integrated from heterogeneous sources, data quality checks and reuse of project software resources. The attributes and functionalities of the proposed solution align with the requirements common to small firms with limited information technology skill and budget. Consequently, this solution can be of great benefit for their data projects.},
	doi = {10.1680/jmapl.17.00052},
	url = {https://doi.org/10.1680%2Fjmapl.17.00052},
	year = 2018,
	month = jun,
	publisher = {Thomas Telford Ltd.},
	volume = {171},
	number = {3},
	pages = {111--125},
	author = {Abiodun Akinyemi and Ming Sun and Alasdair J G Gray},
	title = {An ontology-based data integration framework for construction information management},
	journal = {Proceedings of the Institution of Civil Engineers - Management, Procurement and Law}
}

@article{Harding:GtoPdb:NAR2018,
  abstract  = {The IUPHAR/BPS Guide to PHARMACOLOGY (GtoPdb, www.guidetopharmacology.org) and its precursor IUPHAR-DB, have captured expert-curated interactions between targets and ligands from selected papers in pharmacology and drug discovery since 2003. This resource continues to be developed in conjunction with the International Union of Basic and Clinical Pharmacology (IUPHAR) and the British Pharmacological Society (BPS). As previously described, our unique model of content selection and quality control is based on 96 target-class subcommittees comprising 512 scientists collaborating with in-house curators. This update describes content expansion, new features and interoperability improvements introduced in the 10 releases since August 2015. Our relationship matrix now describes ∼9000 ligands, ∼15 000 binding constants, ∼6000 papers and ∼1700 human proteins. As an important addition, we also introduce our newly funded project for the Guide to IMMUNOPHARMACOLOGY (GtoImmuPdb, www.guidetoimmunopharmacology.org). This has been ‘forked’ from the well-established GtoPdb data model and expanded into new types of data related to the immune system and inflammatory processes. This includes new ligands, targets, pathways, cell types and diseases for which we are recruiting new IUPHAR expert committees. Designed as an immunopharmacological gateway, it also has an emphasis on potential therapeutic interventions.},
  author    = {Simon D. Harding and
               Joanna L. Sharman and
               Elena Faccenda and
               Christopher Southan and
               Adam J. Pawson and
               Sam Ireland and
               Alasdair J. G. Gray and
               Liam Bruce and
               Stephen P. H. Alexander and
               Stephen Anderton and
               Clare Bryant and
               Anthony P. Davenport and
               Christian Doerig and
               Doriano Fabbro and
               Francesca Levi{-}Schaffer and
               Michael Spedding and
               Jamie A. Davies and
               Nc{-}Iuphar},
  title     = {The {IUPHAR/BPS} Guide to {PHARMACOLOGY} in 2018: updates and expansion
               to encompass the new guide to {IMMUNOPHARMACOLOGY}},
  journal   = {Nucleic Acids Research},
  volume    = {46},
  number    = {Database-Issue},
  pages     = {D1091--D1106},
  year      = {2018},
  url       = {https://doi.org/10.1093/nar/gkx1121},
  doi       = {10.1093/nar/gkx1121}
}

@article{Wilkinson:InteropFAIRWebTech:PeerJ2017,
  author    = {Mark D. Wilkinson and
               Ruben Verborgh and
               Luiz Olavo Bonino da Silva Santos and
               Tim Clark and
               Morris A. Swertz and
               Fleur D. L. Kelpin and
               Alasdair J. G. Gray and
               Erik A. Schultes and
               Erik M. van Mulligen and
               Paolo Ciccarese and
               Arnold Kuzniar and
               Anand Gavai and
               Mark Thompson and
               Rajaram Kaliyaperumal and
               Jerven T. Bolleman and
               Michel Dumontier},
  title     = {Interoperability and FAIRness through a novel combination of Web technologies},
  journal   = {PeerJ Computer Science},
  volume    = {3},
  pages     = {e110},
  year      = {2017},
  url       = {https://doi.org/10.7717/peerj-cs.110},
  doi       = {10.7717/peerj-cs.110},
  abstract  = {Data in the life sciences are extremely diverse and are stored in a broad spectrum of repositories ranging from those designed for particular data types (such as KEGG for pathway data or UniProt for protein data) to those that are general-purpose (such as FigShare, Zenodo, Dataverse or EUDAT). These data have widely different levels of sensitivity and security considerations. For example, clinical observations about genetic mutations in patients are highly sensitive, while observations of species diversity are generally not. The lack of uniformity in data models from one repository to another, and in the richness and availability of metadata descriptions, makes integration and analysis of these data a manual, time-consuming task with no scalability. Here we explore a set of resource-oriented Web design patterns for data discovery, accessibility, transformation, and integration that can be implemented by any general- or special-purpose repository as a means to assist users in finding and reusing their data holdings. We show that by using off-the-shelf technologies, interoperability can be achieved at the level of an individual spreadsheet cell. We note that the behaviours of this architecture compare favourably to the desiderata defined by the FAIR Data Principles, and can therefore represent an exemplar implementation of those principles. The proposed interoperability design patterns may be used to improve discovery and integration of both new and legacy data, maximizing the utility of all scholarly outputs.}
}

@article{Dumontier:HCLS-datadesc:PeerJ2016,
  abstract={Access to consistent, high-quality metadata is critical to finding, understanding, and reusing scientific data. However, while there are many relevant vocabularies for the annotation of a dataset, none sufficiently captures all the necessary metadata. This prevents uniform indexing and querying of dataset repositories. Towards providing a practical guide for producing a high quality description of biomedical datasets, the W3C Semantic Web for Health Care and the Life Sciences Interest Group (HCLSIG) identified Resource Description Framework (RDF) vocabularies that could be used to specify common metadata elements and their value sets. The resulting guideline covers elements of description, identification, attribution, versioning, provenance, and content summarization. This guideline reuses existing vocabularies, and is intended to meet key functional requirements including indexing, discovery, exchange, query, and retrieval of datasets, thereby enabling the publication of FAIR data. The resulting metadata profile is generic and could be used by other domains with an interest in providing machine readable descriptions of versioned datasets.},
  title={The health care and life sciences community profile for dataset descriptions},
  author={Dumontier, Michel and Gray, Alasdair JG and Marshall, M Scott and Alexiev, Vladimir and Ansell, Peter and Bader, Gary and Baran, Joachim and Bolleman, Jerven T and Callahan, Alison and Cruz-Toledo, Jos{\'e} and others},
  journal={PeerJ},
  volume={4},
  pages={e2331},
  year={2016},
  month=aug,
  url={https://doi.org/10.7717/peerj.2331},
  doi={10.7717/peerj.2331},
  publisher={PeerJ Inc.}
}

@article{Playford:AdminSocSciRep:BDS2016,
author = {Christopher J Playford and Vernon Gayle and Roxanne Connelly and Alasdair JG Gray},
title ={Administrative social science data: The challenge of reproducible research},
journal = {Big Data and Society},
volume = {3},
number = {2},
pages = {2053951716684143},
year = {2016},
doi = {10.1177/2053951716684143},
URL = {https://doi.org/10.1177/2053951716684143},
eprint = {https://doi.org/10.1177/2053951716684143},
abstract = {Powerful new social science data resources are emerging. One particularly important source is administrative data, which were originally collected for organisational purposes but often contain information that is suitable for social science research. In this paper we outline the concept of reproducible research in relation to micro-level administrative social science data. Our central claim is that a planned and organised workflow is essential for high quality research using micro-level administrative social science data. We argue that it is essential for researchers to share research code, because code sharing enables the elements of reproducible research. First, it enables results to be duplicated and therefore allows the accuracy and validity of analyses to be evaluated. Second, it facilitates further tests of the robustness of the original piece of research. Drawing on insights from computer science and other disciplines that have been engaged in e-Research we discuss and advocate the use of Git repositories to provide a useable and effective solution to research code sharing and rendering social science research using micro-level administrative data reproducible.}
}

@article{Wilkinson:FAIRPrinciples:SciData2016,
  title={The FAIR Guiding Principles for scientific data management and stewardship},
  author={Wilkinson, Mark D and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E and others},
  journal={Scientific data},
  volume={3},
  number={1},
  pages={1--9},
  year={2016},
  publisher={Nature Publishing Group},
  doi={10.1038/sdata.2016.18},
  abstract={There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.}
}

@article{DBLP:journals/internet/Gray14,
  author    = {Alasdair J. G. Gray},
  title     = {Dataset Descriptions for Linked Data Systems},
  journal   = {{IEEE} Internet Computing},
  volume    = {18},
  number    = {4},
  pages     = {66--69},
  year      = {2014},
  url       = {https://doi.org/10.1109/MIC.2014.66},
  doi       = {10.1109/MIC.2014.66}
}

@article{Gray:OPS:SWJ,
  abstract = {The discovery of new medicines requires pharmacologists to interact with a number of information sources ranging from tabular data to scientific papers, and other specialized formats. In this application report, we describe a linked data platform for integrating multiple pharmacology datasets that form the basis for several drug discovery applications. The functionality offered by the platform has been drawn from a collection of prioritised drug discovery business questions created as part of the Open PHACTS project, a collaboration of research institutions and major pharmaceutical companies. We describe the architecture of the platform focusing on seven design decisions that drove its development with the aim of informing others developing similar software in this or other domains. The utility of the platform is demonstrated by the variety of drug discovery applications being built to access the integrated data.},
  author    = {Alasdair J. G. Gray and
               Paul T. Groth and
               Antonis Loizou and
               Sune Askjaer and
               Christian Y. A. Brenninkmeijer and
               Kees Burger and
               Christine Chichester and
               Chris T. A. Evelo and
               Carole A. Goble and
               Lee Harland and
               Steve Pettifer and
               Mark Thompson and
               Andra Waagmeester and
               Antony J. Williams},
  title     = {Applying linked data approaches to pharmacology: Architectural decisions
               and implementation},
  journal   = {Semantic Web Journal},
  volume    = {5},
  number    = {2},
  pages     = {101--113},
  year      = {2014},
  url       = {https://doi.org/10.3233/SW-2012-0088},
  doi       = {10.3233/SW-2012-0088}
}

@article{DBLP:journals/ws/GrothLGGHP14,
  author    = {Paul T. Groth and
               Antonis Loizou and
               Alasdair J. G. Gray and
               Carole A. Goble and
               Lee Harland and
               Steve Pettifer},
  title     = {API-centric Linked Data integration: The Open {PHACTS} Discovery Platform
               case study},
  journal   = {Journal of Web Semantics},
  volume    = {29},
  pages     = {12--18},
  year      = {2014},
  url       = {https://doi.org/10.1016/j.websem.2014.03.003},
  doi       = {10.1016/j.websem.2014.03.003}
}

@article{Ciccarese:PAV:JoBS2013,
  abstract  = {Background
  Provenance is a critical ingredient for establishing trust of published scientific content. This is true whether we are considering a data set, a computational workflow, a peer-reviewed publication or a simple scientific claim with supportive evidence. Existing vocabularies such as Dublin Core Terms (DC Terms) and the W3C Provenance Ontology (PROV-O) are domain-independent and general-purpose and they allow and encourage for extensions to cover more specific needs. In particular, to track authoring and versioning information of web resources, PROV-O provides a basic methodology but not any specific classes and properties for identifying or distinguishing between the various roles assumed by agents manipulating digital artifacts, such as author, contributor and curator.
  
  Results
  We present the Provenance, Authoring and Versioning ontology (PAV, namespace http://purl.org/pav/): a lightweight ontology for capturing “just enough” descriptions essential for tracking the provenance, authoring and versioning of web resources. We argue that such descriptions are essential for digital scientific content. PAV distinguishes between contributors, authors and curators of content and creators of representations in addition to the provenance of originating resources that have been accessed, transformed and consumed. We explore five projects (and communities) that have adopted PAV illustrating their usage through concrete examples. Moreover, we present mappings that show how PAV extends the W3C PROV-O ontology to support broader interoperability.
  
  Method
  The initial design of the PAV ontology was driven by requirements from the AlzSWAN project with further requirements incorporated later from other projects detailed in this paper. The authors strived to keep PAV lightweight and compact by including only those terms that have demonstrated to be pragmatically useful in existing applications, and by recommending terms from existing ontologies when plausible.
  
  Discussion
  We analyze and compare PAV with related approaches, namely Provenance Vocabulary (PRV), DC Terms and BIBFRAME. We identify similarities and analyze differences between those vocabularies and PAV, outlining strengths and weaknesses of our proposed model. We specify SKOS mappings that align PAV with DC Terms. We conclude the paper with general remarks on the applicability of PAV.},
  author    = {Paolo Ciccarese and
               Stian Soiland{-}Reyes and
               Khalid Belhajjame and
               Alasdair J. G. Gray and
               Carole A. Goble and
               Tim Clark},
  title     = {{PAV} ontology: provenance, authoring and versioning},
  journal   = {Journal of Biomedical Semantics},
  volume    = {4},
  pages     = {37},
  year      = {2013},
  url       = {https://doi.org/10.1186/2041-1480-4-37},
  doi       = {10.1186/2041-1480-4-37}
}

@article{Jackman:2013Processing-Online-Crop-Disease,
  title = "Processing online crop disease warning information via sensor networks using ISA ontologies",
  abstract = "Growing demand for food is driving the need for higher crop yields globally. Correctly anticipating the onset of damaging crop diseases is essential to achieve this goal. Considerable efforts have been made recently to develop early warning systems. However, these methods lack a direct and online measurement of the spores that attack crops. A novel disease information network has been implemented and deployed. Spore sensors have been developed and deployed. The measurements from these sensors are combined with similar measurements of important local weather readings to generate estimates of crop disease risk. It is combined with other crop disease information allowing overall local disease risk assessments and forecasts to be made. The resulting data is published through a SPARQL endpoint to support reuse and connection into the linked data cloud.",
  keywords = "Crop disease assessment, Data queries, Investigation study assay, Online sensors, Sensor network, Web semantics",
  author = "Patrick Jackman and Alasdair J G Gray and Andrew Brass and Robert Stevens and Ming Shi and Derek Scuffell and Simon Hammersley and Bruce Grieve",
  year = "2013",
  language = "English",
  volume = "15",
  pages = "243--251",
  journal = "Agricultural Engineering International: CIGR Journal",
  issn = "1682-1130",
  publisher = "International Commission of Agricultural and Biosystems Engineering",
  number = "3",
}

@article{DBLP:journals/dpd/GalpinBGJFP11,
  abstract  = {A wireless sensor network (WSN) can be construed as an intelligent, large-scale device for observing and measuring properties of the physical world. In recent years, the database research community has championed the view that if we construe a WSN as a database (i.e., if a significant aspect of its intelligent behavior is that it can execute declaratively-expressed queries), then one can achieve a significant reduction in the cost of engineering the software that implements a data collection program for the WSN while still achieving, through query optimization, very favorable cost:benefit ratios. This paper describes a query processing framework for WSNs that meets many desiderata associated with the view of WSN as databases. The framework is presented in the form of compiler/optimizer, called SNEE, for a continuous declarative query language over sensed data streams, called SNEEql. SNEEql can be shown to meet the expressiveness requirements of a large class of applications. SNEE can be shown to generate effective and efficient query evaluation plans. More specifically, the paper describes the following contributions: (1) a user-level syntax and physical algebra for SNEEql, an expressive continuous query language over WSNs; (2) example concrete algorithms for physical algebraic operators defined in such a way that the task of deriving memory, time and energy analytical cost-estimation models (CEMs) for them becomes straightforward by reduction to a structural traversal of the pseudocode; (3) CEMs for the concrete algorithms alluded to; (4) an architecture for the optimization of SNEEql queries, called SNEE, building on well-established distributed query processing components where possible, but making enhancements or refinements where necessary to accommodate the WSN context; (5) algorithms that instantiate the components in the SNEE architecture, thereby supporting integrated query planning that includes routing, placement and timing; and (6) an empirical performance evaluation of the resulting framework.},
  author    = {Ixent Galpin and
               Christian Y. A. Brenninkmeijer and
               Alasdair J. G. Gray and
               Farhana Jabeen and
               Alvaro A. A. Fernandes and
               Norman W. Paton},
  title     = {{SNEE:} a query processor for wireless sensor networks},
  journal   = {Distributed and Parallel Databases},
  volume    = {29},
  number    = {1-2},
  pages     = {31--85},
  year      = {2011},
  url       = {https://doi.org/10.1007/s10619-010-7074-3},
  doi       = {10.1007/s10619-010-7074-3}
}

@article{DBLP:journals/sensors/GraySKKKCPGFGFP11,
  author    = {Alasdair J. G. Gray and
               Jason Sadler and
               Oles Kit and
               Kostis Kyzirakos and
               Manos Karpathiotakis and
               Jean-Paul Calbimonte and
               Kevin R. Page and
               Ra{\'{u}}l Garc{\'{\i}}a{-}Castro and
               Alex Frazer and
               Ixent Galpin and
               Alvaro A. A. Fernandes and
               Norman W. Paton and
               {\'{O}}scar Corcho and
               Manolis Koubarakis and
               David De Roure and
               Kirk Martinez and
               Asunci{\'{o}}n G{\'{o}}mez{-}P{\'{e}}rez},
  title     = {A Semantic Sensor Web for Environmental Decision Support Applications},
  journal   = {Sensors},
  volume    = {11},
  number    = {9},
  pages     = {8855--8887},
  year      = {2011},
  url       = {https://doi.org/10.3390/s110908855},
  doi       = {10.3390/s110908855}
}

@article{DBLP:journals/ipm/GrayGHO10,
  abstract  = {Astronomy, like many domains, already has several sets of terminology in general use, referred to as controlled vocabularies. For example, the keywords for tagging journal articles, or the taxonomy of terms used to label image files. These existing vocabularies can be encoded into skos, a W3C proposed recommendation for representing vocabularies on the Semantic Web, so that computer systems can help users to search for and discover resources tagged with vocabulary concepts. However, this requires a search mechanism to go from a user-supplied string to a vocabulary concept.

              In this paper, we present our experiences in implementing the Vocabulary Explorer, a vocabulary search service based on the Terrier Information Retrieval Platform. We investigate the capabilities of existing document weighting models for identifying the correct vocabulary concept for a query. Due to the highly structured nature of a skos encoded vocabulary, we investigate the effects of term weighting (boosting the score of concepts that match on particular fields of a vocabulary concept), and query expansion. We found that the existing document weighting models provided very high quality results, but these could be improved further with the use of term weighting that makes use of the semantic evidence.},
  author    = {Alasdair J. G. Gray and
               Norman Gray and
               Christopher W. Hall and
               Iadh Ounis},
  title     = {Finding the right term: Retrieving and exploring semantic concepts
               in astronomical vocabularies},
  journal   = {Information Processing and Management},
  volume    = {46},
  number    = {4},
  pages     = {470--478},
  year      = {2010},
  Note = {(Alphabetic authorship)},
  url       = {https://doi.org/10.1016/j.ipm.2009.09.004},
  doi       = {10.1016/j.ipm.2009.09.004}
}

@article{Gray:AnsQIncompleteStream:IJWIS2007,
  abstract  = {Purpose
  Distributed data streams are an important topic of current research. In such a setting, data values will be missed, e.g. due to network errors. This paper aims to allow this incompleteness to be detected and overcome with either the user not being affected or the effects of the incompleteness being reported to the user.
  
  Design/methodology/approach
  A model for representing the incomplete information has been developed that captures the information that is known about the missing data. Techniques for query answering involving certain and possible answer sets have been extended so that queries over incomplete data stream histories can be answered.
  
  Findings
  It is possible to detect when a distributed data stream is missing one or more values. When such data values are missing there will be some information that is known about the data and this is stored in an appropriate format. Even when the available data are incomplete, it is possible in some circumstances to answer a query completely. When this is not possible, additional meta‐data can be returned to inform the user of the effects of the incompleteness.
  
  Research limitations/implications
  The techniques and models proposed in this paper have only been partially implemented.
  
  Practical implications
  The proposed system is general and can be applied wherever there is a need to query the history of distributed data streams. The work in this paper enables the system to answer queries when there are missing values in the data.
  
  Originality/value
  This paper presents a general model of how to detect, represent, and answer historical queries over incomplete distributed data streams.},
  author    = {Alasdair J. G. Gray and
               Werner Nutt and
               M. Howard Williams},
  title     = {Answering queries over incomplete data stream histories},
  journal   = {International Journal of Web Information Systems ({IJWIS})},
  volume    = {3},
  number    = {1/2},
  pages     = {41--60},
  year      = {2007},
  url       = {https://doi.org/10.1108/17440080710829216},
  doi       = {10.1108/17440080710829216}
}

@article{Cooke:StreamIntegration:JoDS2005,
  author    = {Andrew W. Cooke and
               Alasdair J. G. Gray and
               Werner Nutt},
  title     = {Stream Integration Techniques for Grid Monitoring},
  journal   = {Journal on Data Semantics},
  volume    = {2},
  pages     = {136--175},
  year      = {2005},
  Note = {(Alphabetical authorship, equal responsibility)},
  url       = {https://doi.org/10.1007/978-3-540-30567-5\_6},
  doi       = {10.1007/978-3-540-30567-5\_6}
}

@article{Cooke:RGMA:JoGC2004,
  abstract  = {We have developed and implemented the Relational Grid Monitoring Architecture (R-GMA) as part of the DataGrid project, to provide a flexible information and monitoring service for use by other middleware components and applications.

              R-GMA presents users with a virtual database and mediates queries posed at this database: users pose queries against a global schema and R-GMA takes responsibility for locating relevant sources and returning an answer. R-GMA’s architecture and mechanisms are general and can be used wherever there is a need for publishing and querying information in a distributed environment.
  
              We discuss the requirements, design and implementation of R-GMA as deployed on the DataGrid testbed. We also describe some of the ways in which R-GMA is being used.},
  author    = {Andrew W. Cooke and
               Alasdair J. G. Gray and
               Werner Nutt and
               James Magowan and
               Manfred Oevers and
               Paul Taylor and
               Roney Cordenonsi and
               Rob Byrom and
               Linda Cornwall and
               Abdeslem Djaoui and
               Laurence Field and
               Steve Fisher and
               Steve Hicks and
               Jason Leake and
               Robin Middleton and
               Antony J. Wilson and
               Xiaomei Zhu and
               Norbert Podhorszki and
               Brian A. Coghlan and
               Stuart Kenny and
               David O'Callaghan and
               John Ryan},
  title     = {The Relational Grid Monitoring Architecture: Mediating Information
               about the Grid},
  journal   = {Journal of Grid Computing},
  volume    = {2},
  number    = {4},
  pages     = {323--339},
  year      = {2004},
  Note = {(Alphabetical authorship by site, Heriot-Watt authored paper)},
  url       = {https://doi.org/10.1007/s10723-005-0151-6},
  doi       = {10.1007/s10723-005-0151-6}
}
